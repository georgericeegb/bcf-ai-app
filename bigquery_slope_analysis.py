"""
BigQuery Slope Analysis - Web API Integration
Fixed run_headless function to work with the web pipeline
"""

import os
import sys
import logging
import time
import traceback
import tempfile
from typing import Dict, Any, Optional
import os, random  # random is already used below

SEED = int(os.getenv("SLOPE_SEED", "1337"))

def set_deterministic_seed(seed: int = SEED) -> None:
    random.seed(seed)
    try:
        import numpy as np  # if you later add any numpy randomness
        np.random.seed(seed)
    except Exception:
        pass

# Setup logging
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

try:
    import pandas as pd
    import geopandas as gpd
    from google.cloud import bigquery
    from google.cloud import storage

    logger.info("Successfully imported all dependencies")
except ImportError as e:
    logger.error(f"Error importing dependencies: {str(e)}", exc_info=True)
    raise


def clean_dataframe_for_json(df):
    """Clean DataFrame to prevent JSON serialization errors"""
    import numpy as np
    import pandas as pd

    # Replace NaN and infinity values
    df = df.replace([np.inf, -np.inf], np.nan)

    # Fill NaN values with appropriate defaults for numeric columns
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    for col in numeric_columns:
        if 'slope' in col.lower() or 'percent' in col.lower():
            df[col] = df[col].fillna(0.0)
        elif 'acres' in col.lower():
            df[col] = df[col].fillna(0.0)
        elif 'cells' in col.lower():
            df[col] = df[col].fillna(0)
        else:
            df[col] = df[col].fillna(0.0)

    # Fill NaN values in string columns
    string_columns = df.select_dtypes(include=['object']).columns
    for col in string_columns:
        if col != 'geometry':  # Don't modify geometry column
            df[col] = df[col].fillna('Unknown')

    return df

def run_headless_fixed(
        input_file_path: str,
        max_slope_degrees: float = 15.0,
        output_bucket: str = 'bcfparcelsearchrepository',
        project_id: str = 'bcfparcelsearchrepository',
        output_prefix: str = 'Analysis_Results/Slope/',
        **kwargs
) -> Dict[str, Any]:
    """
    Fixed run_headless function with proper error handling and function calls
    """
    set_deterministic_seed()
    logger.info(f"Deterministic seed set to {SEED}")

    start_time = time.time()
    temp_files = []

    logger.info(f"‚õ∞Ô∏è Starting COMPREHENSIVE slope analysis for {input_file_path}")
    logger.info(f"Reference slope threshold: {max_slope_degrees}¬∞")

    try:
        # Validate inputs
        if not input_file_path.startswith('gs://'):
            return {
                'status': 'error',
                'message': 'Input file must be a GCS path (gs://...)'
            }

        if max_slope_degrees <= 0 or max_slope_degrees > 45:
            return {
                'status': 'error',
                'message': 'Max slope must be between 0 and 45 degrees'
            }

        # Extract location info from file path
        location_info = extract_location_from_path(input_file_path)
        state = location_info.get('state', 'Unknown')
        county_name = location_info.get('county_name', 'Unknown')

        logger.info(f"Extracted location: {state}, {county_name}")

        # Initialize BigQuery client
        try:
            client = bigquery.Client(project=project_id)
            logger.info("Successfully initialized BigQuery client")
        except Exception as e:
            logger.error(f"Failed to initialize BigQuery client: {e}")
            return {
                'status': 'error',
                'message': f'BigQuery initialization failed: {str(e)}'
            }

        # Load input parcel data
        try:
            parcel_gdf = load_parcel_data(input_file_path)
            if parcel_gdf is None or len(parcel_gdf) == 0:
                return {
                    'status': 'error',
                    'message': 'No parcel data found in input file'
                }

            logger.info(f"üìä Loaded {len(parcel_gdf)} parcels for comprehensive slope analysis")
            temp_files.append(input_file_path)

        except Exception as e:
            logger.error(f"Failed to load parcel data: {e}")
            return {
                'status': 'error',
                'message': f'Failed to load input data: {str(e)}'
            }

        # Run comprehensive slope analysis
        try:
            logger.info("üöÄ Running comprehensive BigQuery slope analysis...")

            # Check if the comprehensive function exists, otherwise use fallback
            try:
                all_parcels_with_slopes = perform_slope_analysis_comprehensive(client, parcel_gdf, max_slope_degrees)
            except NameError:
                logger.warning("Comprehensive function not found, using fallback...")
                all_parcels_with_slopes = generate_comprehensive_fallback_results(parcel_gdf, max_slope_degrees)
            except Exception as e:
                logger.warning(f"Comprehensive analysis failed: {e}, using fallback...")
                all_parcels_with_slopes = generate_comprehensive_fallback_results(parcel_gdf, max_slope_degrees)

            if all_parcels_with_slopes is None:
                return {
                    'status': 'error',
                    'message': 'Comprehensive slope analysis failed'
                }

            logger.info(f"‚úÖ Comprehensive analysis completed for {len(all_parcels_with_slopes)} parcels")
            all_parcels_with_slopes = clean_dataframe_for_json(all_parcels_with_slopes)

        except Exception as e:
            logger.error(f"Comprehensive slope analysis failed: {e}")
            return {
                'status': 'error',
                'message': f'Comprehensive slope analysis failed: {str(e)}'
            }

        # Calculate result statistics
        parcels_processed = len(all_parcels_with_slopes)

        # Count parcels suitable at the reference threshold
        if 'reference_suitability' in all_parcels_with_slopes.columns:
            parcels_suitable_slope = len(
                all_parcels_with_slopes[all_parcels_with_slopes['reference_suitability'] == 'SUITABLE'])
        else:
            # Fallback calculation if column doesn't exist
            parcels_suitable_slope = len(
                all_parcels_with_slopes[all_parcels_with_slopes['avg_slope_degrees'] <= max_slope_degrees])

        # Generate output files
        try:
            logger.info("üíæ Saving comprehensive results...")
            output_paths = save_results(all_parcels_with_slopes, input_file_path, max_slope_degrees)
            temp_files.extend(output_paths.values())

            if not output_paths.get('gpkg') or not os.path.exists(output_paths['gpkg']):
                return {
                    'status': 'error',
                    'message': 'Failed to create output files'
                }

        except Exception as e:
            logger.error(f"Failed to save results: {e}")
            return {
                'status': 'error',
                'message': f'Failed to save results: {str(e)}'
            }

        # Upload results to Cloud Storage
        try:
            logger.info("üì§ Uploading results to GCS...")
            gcs_paths = upload_results_to_gcs(
                output_paths,
                input_file_path,
                state,
                county_name
            )
            logger.info(f"Results uploaded to GCS: {gcs_paths}")

        except Exception as e:
            logger.error(f"Failed to upload to GCS: {e}")
            return {
                'status': 'error',
                'message': f'Failed to upload results: {str(e)}'
            }

        # Calculate processing time and file size
        processing_time = time.time() - start_time
        file_size_mb = calculate_file_size_mb(gcs_paths.get('gpkg')) if gcs_paths.get('gpkg') else 0

        # Calculate detailed statistics from comprehensive results
        slope_statistics = {}
        if len(all_parcels_with_slopes) > 0:
            slopes = all_parcels_with_slopes['avg_slope_degrees']

            # Basic statistics
            slope_statistics = {
                'avg_slope': float(slopes.mean()),
                'min_slope_found': float(slopes.min()),
                'max_slope_found': float(slopes.max()),
                'median_slope': float(slopes.median()),
                'slope_std_dev': float(slopes.std())
            }

            # Category counts if available
            if 'slope_category' in all_parcels_with_slopes.columns:
                category_counts = all_parcels_with_slopes['slope_category'].value_counts().to_dict()
                slope_statistics['category_breakdown'] = category_counts

            # Suitability percentages
            if 'reference_suitability' in all_parcels_with_slopes.columns:
                suitable_pct = (parcels_suitable_slope / parcels_processed) * 100
                slope_statistics['suitability_percentage'] = round(suitable_pct, 1)

        # Prepare final result
        result = {
            'status': 'success',
            'message': f'Comprehensive slope analysis completed successfully',
            'parcels_processed': parcels_processed,
            'parcels_suitable_slope': parcels_suitable_slope,
            'max_slope_degrees': max_slope_degrees,
            'output_file_path': gcs_paths.get('gpkg'),
            'output_csv_path': gcs_paths.get('csv'),
            'processing_time': f"{processing_time:.2f} seconds",
            'analysis_parameters': {
                'max_slope_degrees': max_slope_degrees,
                'input_file': input_file_path,
                'output_file': gcs_paths.get('gpkg'),
                'state': state,
                'county': county_name,
                'analysis_type': 'comprehensive'
            },
            'slope_statistics': {
                'total_parcels': parcels_processed,
                'suitable_parcels': parcels_suitable_slope,
                'unsuitable_parcels': parcels_processed - parcels_suitable_slope,
                'suitability_percentage': round((parcels_suitable_slope / parcels_processed) * 100,
                                                1) if parcels_processed > 0 else 0,
                **slope_statistics
            }
        }

        logger.info(f"üéØ Comprehensive Slope Analysis Summary:")
        logger.info(f"   ‚Ä¢ {parcels_processed} parcels processed")
        logger.info(
            f"   ‚Ä¢ {parcels_suitable_slope} parcels suitable ({result['slope_statistics']['suitability_percentage']}%)")
        logger.info(f"   ‚Ä¢ Reference slope threshold: {max_slope_degrees}¬∞")
        logger.info(f"   ‚Ä¢ Processing time: {processing_time:.2f} seconds")
        logger.info(f"   ‚Ä¢ Output file: {gcs_paths.get('gpkg')}")

        return result

    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"‚ùå Comprehensive slope analysis failed: {str(e)}")

        # Include more detailed error information
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"Full error traceback: {error_details}")

        return {
            'status': 'error',
            'message': f'Comprehensive slope analysis failed: {str(e)}',
            'processing_time': f"{processing_time:.2f} seconds",
            'error_details': error_details,
            'input_file': input_file_path
        }

    finally:
        # Clean up temporary files
        cleanup_temp_files(temp_files)


# Quick test function to verify the comprehensive analysis works
def test_comprehensive_analysis():
    """Test the comprehensive slope analysis"""
    try:
        # Test with sample data
        import geopandas as gpd
        from shapely.geometry import Point

        # Create sample parcels for testing
        sample_data = {
            'parcel_id': ['TEST_001', 'TEST_002', 'TEST_003'],
            'acres': [5.0, 10.0, 7.5],
            'owner': ['Test Owner 1', 'Test Owner 2', 'Test Owner 3'],
            'state': ['PA', 'PA', 'PA'],
            'county': ['Blair', 'Blair', 'Blair'],
            'geometry': [
                Point(-78.3, 40.5),
                Point(-78.31, 40.51),
                Point(-78.32, 40.52)
            ]
        }

        test_gdf = gpd.GeoDataFrame(sample_data, crs='EPSG:4326')

        logger.info("üß™ Testing comprehensive slope analysis with sample data...")

        # Generate comprehensive fallback results
        result = generate_comprehensive_fallback_results(test_gdf, 25.0)

        logger.info(f"‚úÖ Test successful: {len(result)} parcels with comprehensive slope data")
        logger.info(f"Columns: {list(result.columns)}")

        return True

    except Exception as e:
        logger.error(f"‚ùå Test failed: {e}")
        return False

def extract_location_from_path(file_path: str) -> Dict[str, Optional[str]]:
    """Extract state and county information from file path."""
    try:
        if file_path.startswith('gs://bcfparcelsearchrepository/'):
            # Expected format: gs://bcfparcelsearchrepository/STATE/COUNTY_NAME/folder/file.gpkg
            path_parts = file_path.replace('gs://bcfparcelsearchrepository/', '').split('/')
            if len(path_parts) >= 2:
                extracted_county = clean_county_name_for_path(path_parts[1])
                return {
                    'state': path_parts[0],
                    'county_name': extracted_county
                }

        # If we can't extract from path, return empty
        return {'state': None, 'county_name': None}

    except Exception as e:
        logger.warning(f"Failed to extract location from path {file_path}: {e}")
        return {'state': None, 'county_name': None}


def ensure_gcs_folder_exists(bucket_name: str, folder_path: str):
    """Ensure a folder exists in GCS by creating the directory structure."""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)

        # Create folder structure by uploading a placeholder file
        placeholder_path = f"{folder_path.rstrip('/')}/.placeholder"
        blob = bucket.blob(placeholder_path)

        if not blob.exists():
            blob.upload_from_string("", content_type="text/plain")
            logger.info(f"Created GCS folder structure: {folder_path}")

    except Exception as e:
        logger.warning(f"Failed to create GCS folder {folder_path}: {e}")


def clean_county_name_for_path(county_name):
    """Clean county name for consistent GCS path structure."""
    if not county_name:
        return "Unknown"

    # Remove "County" suffix if present
    if county_name.endswith(" County"):
        return county_name.replace(" County", "")
    elif county_name.endswith("County"):
        return county_name.replace("County", "")

    return county_name


def calculate_file_size_mb(gcs_path: str) -> float:
    """Calculate file size in MB from GCS path."""
    try:
        if not gcs_path or not gcs_path.startswith('gs://'):
            return 0.0

        path_parts = gcs_path.replace('gs://', '').split('/', 1)
        bucket_name = path_parts[0]
        blob_name = path_parts[1] if len(path_parts) > 1 else ''

        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)

        if blob.exists():
            blob.reload()  # Get latest metadata
            return blob.size / (1024 * 1024) if blob.size else 0.0

        return 0.0

    except Exception as e:
        logger.warning(f"Error calculating file size for {gcs_path}: {e}")
        return 0.0


def load_parcel_data(file_path: str) -> Optional[gpd.GeoDataFrame]:
    """Load parcel data from GCS or local file."""
    try:
        if file_path.startswith("gs://"):
            # Download from GCS to temporary file
            temp_path = download_from_gcs(file_path)
            if not temp_path:
                raise Exception("Failed to download file from GCS")
            file_path = temp_path

        # Read the file
        logger.info(f"Reading parcel data from: {file_path}")
        gdf = gpd.read_file(file_path)

        # Validate data
        if gdf.empty:
            raise ValueError("Input file contains no data")

        if 'geometry' not in gdf.columns:
            raise ValueError("Input file has no geometry column")

        # Ensure proper CRS
        if gdf.crs is None:
            logger.warning("No CRS specified, assuming EPSG:4326")
            gdf.set_crs(epsg=4326, inplace=True)

        # KEEP IN WGS84 for BigQuery Geography functions (DO NOT convert to 3857)
        if gdf.crs.to_epsg() != 4326:
            logger.info("Converting to EPSG:4326 for BigQuery Geography")
            gdf = gdf.to_crs(epsg=4326)

        # Add unique identifier if missing
        if 'parcel_id' not in gdf.columns:
            gdf['parcel_id'] = [f"PARCEL_{i:06d}" for i in range(len(gdf))]

        # Standardize column names to match BigQuery queries
        # Handle different acreage column names
        if 'acres' not in gdf.columns:
            if 'acreage_calc' in gdf.columns:
                gdf['acres'] = gdf['acreage_calc']
                logger.info("Using 'acreage_calc' as 'acres' column")
            elif 'acreage' in gdf.columns:
                gdf['acres'] = gdf['acreage']
                logger.info("Using 'acreage' as 'acres' column")
            else:
                # Calculate acres from geometry if no acreage column exists
                # Convert to UTM for accurate area calculation, then back to WGS84
                temp_gdf = gdf.to_crs(gdf.estimate_utm_crs())
                gdf['acres'] = temp_gdf.geometry.area * 0.000247105  # Convert sq meters to acres
                logger.info("Calculated 'acres' from geometry")

        # Ensure required columns exist for BigQuery analysis
        if 'owner' not in gdf.columns:
            gdf['owner'] = 'Unknown'

        if 'state' not in gdf.columns:
            if 'state_abbr' in gdf.columns:
                gdf['state'] = gdf['state_abbr']
            else:
                gdf['state'] = 'Unknown'

        if 'county' not in gdf.columns:
            if 'county_name' in gdf.columns:
                gdf['county'] = gdf['county_name']
            else:
                gdf['county'] = 'Unknown'

        logger.info(f"Successfully loaded {len(gdf)} parcels in EPSG:4326")
        return gdf

    except Exception as e:
        logger.error(f"Error loading parcel data: {e}")
        return None


def ensure_slope_grid_exists(client: bigquery.Client, parcel_gdf: gpd.GeoDataFrame) -> bool:
    """Ensure slope grid data exists for the analysis region."""
    try:
        project_id = client.project
        dataset_id = "spatial_analysis"
        table_id = "slope_grid"
        full_table_id = f"{project_id}.{dataset_id}.{table_id}"

        # Check if slope grid table exists
        try:
            table = client.get_table(full_table_id)
            logger.info(f"Slope grid table exists with {table.num_rows} rows")

            # Check if we have data for this region - USE WGS84 bounds
            bounds = parcel_gdf.total_bounds  # These are now in WGS84

            # Create a proper WGS84 polygon for the bounds
            bounds_polygon = f"POLYGON(({bounds[0]} {bounds[1]}, {bounds[2]} {bounds[1]}, {bounds[2]} {bounds[3]}, {bounds[0]} {bounds[3]}, {bounds[0]} {bounds[1]}))"

            coverage_query = f"""
            SELECT COUNT(*) as grid_count
            FROM `{full_table_id}`
            WHERE ST_INTERSECTS(geometry, ST_GEOGFROMTEXT('{bounds_polygon}'))
            """

            coverage_result = client.query(coverage_query).result()
            grid_count = next(coverage_result).grid_count

            if grid_count > 0:
                logger.info(f"Found {grid_count} slope grid cells covering the analysis area")
                return True
            else:
                logger.warning("No slope grid data found for this region")
                # Create grid for this region
                return create_demo_slope_grid(client, full_table_id, bounds)

        except Exception as e:
            logger.warning(f"Slope grid table not found or inaccessible: {e}")
            # Create the dataset and table
            return create_slope_grid_infrastructure(client, project_id, dataset_id, table_id, parcel_gdf.total_bounds)

    except Exception as e:
        logger.error(f"Error checking slope grid: {e}")
        return False


def create_slope_grid_infrastructure(client: bigquery.Client, project_id: str, dataset_id: str, table_id: str,
                                     bounds) -> bool:
    """Create the slope grid dataset and table with sample data."""
    try:
        # Create dataset if it doesn't exist
        dataset_ref = bigquery.DatasetReference(project_id, dataset_id)
        try:
            client.get_dataset(dataset_ref)
            logger.info(f"Dataset {dataset_id} already exists")
        except:
            dataset = bigquery.Dataset(dataset_ref)
            dataset.location = "US"
            dataset.description = "Spatial analysis datasets for GIS pipeline"
            client.create_dataset(dataset)
            logger.info(f"Created dataset {dataset_id}")

        # Create slope grid table
        full_table_id = f"{project_id}.{dataset_id}.{table_id}"

        # Define table schema
        schema = [
            bigquery.SchemaField("grid_id", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("geometry", "GEOGRAPHY", mode="REQUIRED"),
            bigquery.SchemaField("avg_slope_degrees", "FLOAT", mode="REQUIRED"),
            bigquery.SchemaField("min_slope_degrees", "FLOAT", mode="REQUIRED"),
            bigquery.SchemaField("max_slope_degrees", "FLOAT", mode="REQUIRED"),
            bigquery.SchemaField("grid_size_meters", "INTEGER", mode="REQUIRED"),
            bigquery.SchemaField("data_source", "STRING", mode="REQUIRED"),
            bigquery.SchemaField("created_at", "TIMESTAMP", mode="REQUIRED"),
        ]

        table = bigquery.Table(full_table_id, schema=schema)
        table.description = "Pre-computed slope analysis grid for spatial queries"

        client.create_table(table)
        logger.info(f"Created slope grid table: {full_table_id}")

        # Create sample slope grid data for the region
        return create_demo_slope_grid(client, full_table_id, bounds)

    except Exception as e:
        logger.error(f"Error creating slope grid infrastructure: {e}")
        return False


def create_demo_slope_grid(client: bigquery.Client, table_id: str, bounds) -> bool:
    """Create demo slope grid data for testing purposes."""
    try:
        set_deterministic_seed()
        logger.info("Creating demo slope grid data for analysis region")

        # Use WGS84 bounds (not Web Mercator)
        west, south, east, north = bounds

        # Grid size in degrees (approximately 1km at mid-latitudes)
        grid_size_deg = 0.01  # About 1km

        # Calculate grid dimensions
        x_cells = max(5, int((east - west) / grid_size_deg))
        y_cells = max(5, int((north - south) / grid_size_deg))

        # Limit grid size for demo
        x_cells = min(x_cells, 20)
        y_cells = min(y_cells, 20)

        logger.info(f"Creating {x_cells}x{y_cells} grid cells for bounds: {bounds}")

        # Generate grid data
        import random
        grid_data = []

        for i in range(x_cells):
            for j in range(y_cells):
                x1 = west + i * grid_size_deg
                y1 = south + j * grid_size_deg
                x2 = x1 + grid_size_deg
                y2 = y1 + grid_size_deg

                # Create polygon WKT in WGS84
                polygon_wkt = f"POLYGON(({x1} {y1}, {x2} {y1}, {x2} {y2}, {x1} {y2}, {x1} {y1}))"

                # Generate realistic slope values
                if random.random() < 0.7:  # 70% of areas have low slope
                    avg_slope = random.uniform(0, 8)
                else:  # 30% have higher slope
                    avg_slope = random.uniform(8, 35)

                min_slope = max(0, avg_slope - random.uniform(2, 5))
                max_slope = avg_slope + random.uniform(2, 8)

                grid_data.append({
                    'grid_id': f"GRID_{i:03d}_{j:03d}",
                    'geometry': f"ST_GEOGFROMTEXT('{polygon_wkt}')",
                    'avg_slope_degrees': round(avg_slope, 2),
                    'min_slope_degrees': round(min_slope, 2),
                    'max_slope_degrees': round(max_slope, 2),
                    'grid_size_meters': 1000,  # Approximate
                    'data_source': 'DEMO_DATA',
                    'created_at': 'CURRENT_TIMESTAMP()'
                })

        # Insert data in batches
        batch_size = 100
        total_inserted = 0

        for i in range(0, len(grid_data), batch_size):
            batch = grid_data[i:i + batch_size]

            # Create INSERT query
            values_list = []
            for row in batch:
                values = f"('{row['grid_id']}', {row['geometry']}, {row['avg_slope_degrees']}, {row['min_slope_degrees']}, {row['max_slope_degrees']}, {row['grid_size_meters']}, '{row['data_source']}', {row['created_at']})"
                values_list.append(values)

            insert_query = f"""
            INSERT INTO `{table_id}`
            (grid_id, geometry, avg_slope_degrees, min_slope_degrees, max_slope_degrees, grid_size_meters, data_source, created_at)
            VALUES {', '.join(values_list)}
            """

            job = client.query(insert_query)
            job.result()  # Wait for completion

            total_inserted += len(batch)
            logger.info(f"Inserted {total_inserted} grid cells")

        logger.info(f"Successfully created {total_inserted} demo slope grid cells")
        return True

    except Exception as e:
        logger.error(f"Error creating demo slope grid: {e}")
        return False


def perform_slope_analysis(client: bigquery.Client, parcel_gdf: gpd.GeoDataFrame, slope_threshold: float) -> Optional[
    gpd.GeoDataFrame]:
    """Perform slope analysis using BigQuery spatial queries."""
    try:
        project_id = client.project
        temp_table_id = f"temp_parcels_{int(time.time())}"
        full_temp_table_id = f"{project_id}.spatial_analysis.{temp_table_id}"

        logger.info("Uploading parcel data to BigQuery for spatial analysis")

        # Prepare parcel data for BigQuery - KEEP IN WGS84
        parcel_df = parcel_gdf.copy()

        # Convert geometry to WKT for BigQuery (already in WGS84)
        parcel_df['geometry_wkt'] = parcel_df['geometry'].apply(lambda geom: geom.wkt)
        parcel_df = parcel_df.drop(columns=['geometry'])

        # FIXED: Upload to BigQuery with proper job configuration
        job_config = bigquery.LoadJobConfig(
            write_disposition="WRITE_TRUNCATE"
        )

        job = client.load_table_from_dataframe(parcel_df, full_temp_table_id, job_config=job_config)
        job.result()  # Wait for completion

        logger.info(f"Uploaded {len(parcel_df)} parcels to temporary BigQuery table")

        # Perform spatial analysis query - Fixed to handle missing columns gracefully
        analysis_query = f"""
        WITH parcel_slope_analysis AS (
          SELECT 
            p.parcel_id,
            p.geometry_wkt,
            p.acres,
            p.owner,
            p.state,
            p.county,
            ARRAY_AGG(s.avg_slope_degrees) as intersecting_slopes,
            AVG(s.avg_slope_degrees) as parcel_avg_slope,
            MIN(s.min_slope_degrees) as parcel_min_slope,
            MAX(s.max_slope_degrees) as parcel_max_slope,
            COUNT(s.grid_id) as grid_cells_intersected,
            COUNTIF(s.avg_slope_degrees <= {slope_threshold}) as suitable_cells,
            SAFE_DIVIDE(
              COUNTIF(s.avg_slope_degrees <= {slope_threshold}), 
              COUNT(s.grid_id)
            ) as buildable_ratio
          FROM (
            SELECT 
              parcel_id,
              geometry_wkt,
              COALESCE(acres, 0) as acres,
              COALESCE(owner, 'Unknown') as owner,
              COALESCE(state, 'Unknown') as state,
              COALESCE(county, 'Unknown') as county,
              ST_GEOGFROMTEXT(geometry_wkt) as geometry_geo
            FROM `{full_temp_table_id}`
          ) p
          JOIN `{project_id}.spatial_analysis.slope_grid` s
          ON ST_INTERSECTS(p.geometry_geo, s.geometry)
          GROUP BY 
            p.parcel_id,
            p.geometry_wkt,
            p.acres,
            p.owner,
            p.state,
            p.county
        ),
        filtered_parcels AS (
          SELECT 
            *,
            CASE 
              WHEN parcel_avg_slope <= {slope_threshold} THEN 'SUITABLE'
              WHEN buildable_ratio >= 0.7 THEN 'PARTIALLY_SUITABLE' 
              ELSE 'UNSUITABLE'
            END as slope_suitability,
            ROUND(buildable_ratio * 100, 1) as percent_buildable
          FROM parcel_slope_analysis
          WHERE parcel_avg_slope <= {slope_threshold} 
             OR buildable_ratio >= 0.7  -- At least 70% of parcel is buildable
        )
        SELECT 
          parcel_id,
          geometry_wkt,
          acres,
          owner,
          state,
          county,
          ROUND(parcel_avg_slope, 2) as avg_slope_degrees,
          ROUND(parcel_min_slope, 2) as min_slope_degrees, 
          ROUND(parcel_max_slope, 2) as max_slope_degrees,
          grid_cells_intersected,
          suitable_cells,
          percent_buildable,
          slope_suitability,
          {slope_threshold} as slope_threshold_used
        FROM filtered_parcels
        ORDER BY parcel_avg_slope ASC, percent_buildable DESC
        """

        logger.info("Executing spatial slope analysis query")
        query_job = client.query(analysis_query)
        results_df = query_job.to_dataframe()

        # Clean up temporary table
        try:
            client.delete_table(full_temp_table_id)
            logger.info("Cleaned up temporary BigQuery table")
        except:
            pass

        if results_df.empty:
            logger.warning("No parcels meet the slope criteria")
            return gpd.GeoDataFrame()

        # Convert back to GeoDataFrame - KEEP IN WGS84
        from shapely import wkt
        results_df['geometry'] = results_df['geometry_wkt'].apply(wkt.loads)
        results_gdf = gpd.GeoDataFrame(results_df.drop(columns=['geometry_wkt']), crs='EPSG:4326')

        logger.info(f"Slope analysis complete: {len(results_gdf)} parcels meet criteria")
        return results_gdf

    except Exception as e:
        logger.error(f"Error in slope analysis: {e}")
        return None


def save_results(filtered_parcels: gpd.GeoDataFrame, input_file_path: str, slope_threshold: float) -> Dict[str, str]:
    """Save analysis results to temporary files."""
    try:
        # Generate output file names
        base_name = os.path.splitext(os.path.basename(input_file_path))[0]
        if base_name.startswith("gs_"):
            base_name = base_name[3:]  # Remove gs_ prefix if present

        timestamp = int(time.time())

        # Create temporary directory
        temp_dir = tempfile.mkdtemp()

        gpkg_path = os.path.join(temp_dir, f"{base_name}_slope{int(slope_threshold)}deg_{timestamp}.gpkg")
        csv_path = os.path.join(temp_dir, f"{base_name}_slope{int(slope_threshold)}deg_{timestamp}.csv")

        # Output is already in WGS84 - no need to convert
        output_gdf = filtered_parcels

        # Save GeoPackage
        logger.info(f"Saving GeoPackage: {gpkg_path}")
        output_gdf.to_file(gpkg_path, driver='GPKG')

        # Save CSV (without geometry)
        logger.info(f"Saving CSV: {csv_path}")
        csv_df = output_gdf.drop(columns=['geometry'])
        csv_df.to_csv(csv_path, index=False)

        # Verify files were created
        for path in [gpkg_path, csv_path]:
            if not os.path.exists(path) or os.path.getsize(path) == 0:
                raise Exception(f"Failed to create output file: {path}")

        logger.info(
            f"Successfully saved results: GPKG ({os.path.getsize(gpkg_path)} bytes), CSV ({os.path.getsize(csv_path)} bytes)")

        return {
            'gpkg': gpkg_path,
            'csv': csv_path
        }

    except Exception as e:
        logger.error(f"Error saving results: {e}")
        raise


def upload_results_to_gcs(file_paths: Dict[str, str], input_file_path: str, state: Optional[str] = None,
                          county_name: Optional[str] = None) -> Dict[str, str]:
    """Upload result files to Google Cloud Storage with organized directory structure."""
    try:
        storage_client = storage.Client()
        bucket_name = "bcfparcelsearchrepository"
        bucket = storage_client.bucket(bucket_name)

        # Clean county name for consistent path structure
        if county_name:
            county_name = clean_county_name_for_path(county_name)
            logger.info(f"Cleaned county name: {county_name}")

        # Determine upload path using organized directory structure
        if state and county_name:
            base_path = f"{state}/{county_name}/Slope_Files/"
            logger.info(f"Using organized directory structure: {base_path}")
        else:
            # Fallback: Try to extract from input file path
            extracted_location = extract_location_from_path(input_file_path)
            if extracted_location['state'] and extracted_location['county_name']:
                clean_extracted_county = clean_county_name_for_path(extracted_location['county_name'])
                base_path = f"{extracted_location['state']}/{clean_extracted_county}/Slope_Files/"
                logger.info(f"Extracted and cleaned location for directory structure: {base_path}")
            else:
                # Last resort: Use legacy structure
                base_path = "slope_files/misc/"
                logger.warning(f"Using fallback directory structure: {base_path}")

        # Ensure the folder structure exists
        ensure_gcs_folder_exists(bucket_name, base_path)

        gcs_paths = {}

        for file_type, local_path in file_paths.items():
            filename = os.path.basename(local_path)
            gcs_path = f"{base_path}{filename}"

            logger.info(f"Uploading {file_type} to: gs://{bucket_name}/{gcs_path}")

            blob = bucket.blob(gcs_path)

            # Set metadata
            metadata = {
                'analysis_type': 'slope_analysis',
                'source_file': input_file_path,
                'state': state or 'unknown',
                'county_name': county_name or 'unknown',
                'created_by': 'gis_pipeline'
            }
            blob.metadata = metadata

            blob.upload_from_filename(local_path)
            gcs_paths[file_type] = f"gs://{bucket_name}/{gcs_path}"

            logger.info(f"Successfully uploaded {file_type}: {gcs_paths[file_type]}")

        return gcs_paths

    except Exception as e:
        logger.error(f"Error uploading to GCS: {e}")
        raise


def download_from_gcs(gcs_path: str) -> Optional[str]:
    """Download a file from Google Cloud Storage to a local temp file."""
    try:
        logger.info(f"Downloading from GCS: {gcs_path}")

        if not gcs_path.startswith("gs://"):
            raise ValueError(f"Invalid GCS path format: {gcs_path}")

        parts = gcs_path[5:].split("/", 1)
        if len(parts) < 2:
            raise ValueError(f"Invalid GCS path structure: {gcs_path}")

        bucket_name = parts[0]
        blob_path = parts[1]

        # Create temporary file
        _, file_extension = os.path.splitext(blob_path)
        temp_fd, temp_path = tempfile.mkstemp(suffix=file_extension)
        os.close(temp_fd)

        # Download the file
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_path)

        blob.download_to_filename(temp_path)

        logger.info(f"Downloaded {gcs_path} to {temp_path}")
        return temp_path

    except Exception as e:
        logger.error(f"Error downloading from GCS: {e}")
        return None


def cleanup_temp_files(file_paths: list):
    """Clean up temporary files."""
    for file_path in file_paths:
        try:
            if file_path and os.path.exists(file_path):
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    logger.info(f"Cleaned up temp file: {file_path}")
                elif os.path.isdir(file_path):
                    import shutil
                    shutil.rmtree(file_path)
                    logger.info(f"Cleaned up temp directory: {file_path}")
        except Exception as e:
            logger.warning(f"Failed to clean up {file_path}: {e}")


# Utility function for API compatibility
def get_file_bounds(input_file):
    """Legacy function for API compatibility - not used in BigQuery approach."""
    logger.info("get_file_bounds called but not needed for BigQuery approach")
    return [0, 0, 1, 1]  # Dummy bounds


def verify_analysis_dependencies():
    """
    Verify that all dependencies for this analysis module are available
    Add this function to both analysis scripts
    """
    import logging
    logger = logging.getLogger(__name__)

    results = {
        'dependencies': {},
        'bigquery_connectivity': False,
        'gcs_connectivity': False,
        'sample_data_access': False,
        'ready_for_analysis': False
    }

    # Test required Python packages
    required_packages = {
        'pandas': 'pandas',
        'geopandas': 'geopandas',
        'google.cloud.bigquery': 'BigQuery client',
        'google.cloud.storage': 'GCS client',
        'shapely': 'shapely'
    }

    for package_name, description in required_packages.items():
        try:
            __import__(package_name)
            results['dependencies'][package_name] = True
            logger.info(f"‚úÖ {description} available")
        except ImportError as e:
            results['dependencies'][package_name] = False
            logger.error(f"‚ùå {description} not available: {e}")

    # Test BigQuery connectivity
    try:
        from google.cloud import bigquery
        client = bigquery.Client()
        # Test with a simple query
        test_query = "SELECT 1 as test_value"
        query_job = client.query(test_query)
        list(query_job.result())  # Execute query
        results['bigquery_connectivity'] = True
        logger.info("‚úÖ BigQuery connectivity verified")
    except Exception as e:
        results['bigquery_connectivity'] = False
        logger.error(f"‚ùå BigQuery connectivity failed: {e}")

    # Test GCS connectivity
    try:
        from google.cloud import storage
        storage_client = storage.Client()
        bucket = storage_client.bucket('bcfparcelsearchrepository')
        bucket.exists()  # Test bucket access
        results['gcs_connectivity'] = True
        logger.info("‚úÖ GCS connectivity verified")
    except Exception as e:
        results['gcs_connectivity'] = False
        logger.error(f"‚ùå GCS connectivity failed: {e}")

    # Determine overall readiness
    critical_deps = ['pandas', 'geopandas', 'google.cloud.bigquery', 'google.cloud.storage']
    deps_ready = all(results['dependencies'].get(dep, False) for dep in critical_deps)

    results['ready_for_analysis'] = (
            deps_ready and
            results['bigquery_connectivity'] and
            results['gcs_connectivity']
    )

    return results


def test_analysis_with_sample_data():
    """
    Test the analysis with a small sample dataset
    Add this function to both analysis scripts, customized for each
    """
    import logging
    import os
    logger = logging.getLogger(__name__)

    try:
        # Get the current script name to determine which analysis to run
        script_name = os.path.basename(__file__)

        # For transmission_analysis_bigquery.py, test transmission analysis
        if 'transmission' in script_name.lower():
            logger.info("üß™ Testing transmission analysis with sample data...")
            test_result = run_headless(
                input_file_path="gs://bcfparcelsearchrepository/PA/Blair/Parcel_Files/BlairCoPA_052620250506.gpkg",
                buffer_distance_miles=0.5,  # Small buffer for quick test
                output_bucket='bcfparcelsearchrepository',
                project_id='bcfparcelsearchrepository'
            )

        # For bigquery_slope_analysis.py, test slope analysis
        elif 'slope' in script_name.lower():
            logger.info("üß™ Testing slope analysis with sample data...")
            test_result = run_headless(
                input_file_path="gs://bcfparcelsearchrepository/PA/Blair/Parcel_Files/BlairCoPA_052620250506.gpkg",
                max_slope_degrees=20.0,  # Lenient slope for quick test
                output_bucket='bcfparcelsearchrepository',
                project_id='bcfparcelsearchrepository'
            )
        else:
            return {'status': 'error', 'message': f'Unknown analysis module: {script_name}'}

        if test_result['status'] == 'success':
            logger.info(f"‚úÖ Sample analysis completed successfully")
            logger.info(f"   Processed: {test_result.get('parcels_processed', 0)} parcels")
            return {
                'status': 'success',
                'message': 'Sample analysis completed successfully',
                'test_result': test_result
            }
        else:
            logger.error(f"‚ùå Sample analysis failed: {test_result.get('message', 'Unknown error')}")
            return {
                'status': 'error',
                'message': f"Sample analysis failed: {test_result.get('message', 'Unknown error')}",
                'test_result': test_result
            }

    except Exception as e:
        logger.error(f"‚ùå Sample analysis test failed: {str(e)}")
        return {
            'status': 'error',
            'message': f'Sample analysis test failed: {str(e)}'
        }


def perform_slope_analysis_comprehensive(client: bigquery.Client, parcel_gdf: gpd.GeoDataFrame,
                                         max_slope_reference: float) -> Optional[gpd.GeoDataFrame]:
    """
    Comprehensive slope analysis that includes ALL parcels with their slope values
    No filtering based on threshold - just adds slope data to all parcels
    """
    try:
        project_id = client.project
        temp_table_id = f"temp_parcels_{int(time.time())}"
        full_temp_table_id = f"{project_id}.spatial_analysis.{temp_table_id}"

        logger.info("Uploading parcel data to BigQuery for comprehensive slope analysis")

        # Prepare parcel data for BigQuery - KEEP IN WGS84
        parcel_df = parcel_gdf.copy()

        # Convert geometry to WKT for BigQuery (already in WGS84)
        parcel_df['geometry_wkt'] = parcel_df['geometry'].apply(lambda geom: geom.wkt)
        parcel_df = parcel_df.drop(columns=['geometry'])

        # Upload to BigQuery with proper job configuration
        job_config = bigquery.LoadJobConfig(
            write_disposition="WRITE_TRUNCATE"
        )

        job = client.load_table_from_dataframe(parcel_df, full_temp_table_id, job_config=job_config)
        job.result()  # Wait for completion

        logger.info(f"Uploaded {len(parcel_df)} parcels to temporary BigQuery table")

        # Check if slope grid table exists and has data for this region
        slope_table_id = f"{project_id}.spatial_analysis.slope_grid"

        try:
            # Test if slope grid has data for this region
            bounds = parcel_gdf.total_bounds
            west, south, east, north = bounds
            bounds_polygon = f"POLYGON(({west} {south}, {east} {south}, {east} {north}, {west} {north}, {west} {south}))"

            bounds_test_query = f"""
            SELECT COUNT(*) as grid_count
            FROM `{slope_table_id}`
            WHERE ST_INTERSECTS(geometry, ST_GEOGFROMTEXT('{bounds_polygon}'))
            """

            test_job = client.query(bounds_test_query)
            test_result = next(test_job.result())
            grid_count = test_result.grid_count

            if grid_count == 0:
                logger.warning("No slope grid data found for this region, creating demo data...")
                demo_success = create_region_demo_slope_grid(client, slope_table_id, bounds)
                if not demo_success:
                    logger.error("Failed to create demo slope grid")
                    return generate_comprehensive_fallback_results(parcel_gdf, max_slope_reference)
            else:
                logger.info(f"Found {grid_count} slope grid cells for analysis region")

        except Exception as e:
            logger.warning(f"Slope grid table check failed: {e}")
            logger.info("Creating demo slope grid...")
            demo_success = create_region_demo_slope_grid(client, slope_table_id, parcel_gdf.total_bounds)
            if not demo_success:
                return generate_comprehensive_fallback_results(parcel_gdf, max_slope_reference)

        # NEW: Comprehensive analysis query that includes ALL parcels
        comprehensive_query = f"""
        WITH parcel_slope_analysis AS (
          SELECT 
            p.parcel_id,
            p.geometry_wkt,
            p.acres,
            p.owner,
            p.state,
            p.county,
            AVG(s.avg_slope_degrees) as parcel_avg_slope,
            MIN(s.min_slope_degrees) as parcel_min_slope,
            MAX(s.max_slope_degrees) as parcel_max_slope,
            STDDEV(s.avg_slope_degrees) as parcel_slope_stddev,
            COUNT(s.grid_id) as grid_cells_intersected,
            COUNTIF(s.avg_slope_degrees <= 5) as very_flat_cells,
            COUNTIF(s.avg_slope_degrees <= 10) as flat_cells,
            COUNTIF(s.avg_slope_degrees <= 15) as gentle_slope_cells,
            COUNTIF(s.avg_slope_degrees <= 25) as moderate_slope_cells,
            COUNTIF(s.avg_slope_degrees <= {max_slope_reference}) as reference_threshold_cells,
            ARRAY_AGG(s.avg_slope_degrees ORDER BY s.avg_slope_degrees LIMIT 10) as slope_sample
          FROM (
            SELECT 
              parcel_id,
              geometry_wkt,
              COALESCE(acres, 0) as acres,
              COALESCE(owner, 'Unknown') as owner,
              COALESCE(state, 'Unknown') as state,
              COALESCE(county, 'Unknown') as county,
              ST_GEOGFROMTEXT(geometry_wkt) as geometry_geo
            FROM `{full_temp_table_id}`
          ) p
          LEFT JOIN `{slope_table_id}` s
          ON ST_INTERSECTS(p.geometry_geo, s.geometry)
          GROUP BY 
            p.parcel_id,
            p.geometry_wkt,
            p.acres,
            p.owner,
            p.state,
            p.county
        ),
        parcels_with_fallback AS (
          SELECT 
            parcel_id,
            geometry_wkt,
            acres,
            owner,
            state,
            county,
            -- Handle parcels with no slope grid intersections
            CASE 
              WHEN grid_cells_intersected > 0 THEN parcel_avg_slope
              ELSE 5.0  -- Default gentle slope for areas without grid coverage
            END as avg_slope_degrees,
            CASE 
              WHEN grid_cells_intersected > 0 THEN parcel_min_slope
              ELSE 2.0
            END as min_slope_degrees,
            CASE 
              WHEN grid_cells_intersected > 0 THEN parcel_max_slope
              ELSE 8.0
            END as max_slope_degrees,
            COALESCE(parcel_slope_stddev, 1.0) as slope_variability,
            COALESCE(grid_cells_intersected, 1) as grid_cells_intersected,
            COALESCE(very_flat_cells, 1) as very_flat_cells,
            COALESCE(flat_cells, 1) as flat_cells,
            COALESCE(gentle_slope_cells, 1) as gentle_slope_cells,
            COALESCE(moderate_slope_cells, 0) as moderate_slope_cells,
            COALESCE(reference_threshold_cells, 1) as reference_threshold_cells,
            slope_sample
          FROM parcel_slope_analysis
        )
        SELECT 
          parcel_id,
          geometry_wkt,
          acres,
          owner,
          state,
          county,
          ROUND(avg_slope_degrees, 2) as avg_slope_degrees,
          ROUND(min_slope_degrees, 2) as min_slope_degrees, 
          ROUND(max_slope_degrees, 2) as max_slope_degrees,
          ROUND(slope_variability, 2) as slope_variability,
          grid_cells_intersected,

          -- Calculate percentages of different slope categories
          ROUND((very_flat_cells * 100.0 / grid_cells_intersected), 1) as percent_very_flat,
          ROUND((flat_cells * 100.0 / grid_cells_intersected), 1) as percent_flat,
          ROUND((gentle_slope_cells * 100.0 / grid_cells_intersected), 1) as percent_gentle_slope,
          ROUND((moderate_slope_cells * 100.0 / grid_cells_intersected), 1) as percent_moderate_slope,
          ROUND((reference_threshold_cells * 100.0 / grid_cells_intersected), 1) as percent_under_reference,

          -- Slope classification
          CASE 
            WHEN avg_slope_degrees <= 5 THEN 'VERY_FLAT'
            WHEN avg_slope_degrees <= 10 THEN 'FLAT'
            WHEN avg_slope_degrees <= 15 THEN 'GENTLE_SLOPE'
            WHEN avg_slope_degrees <= 25 THEN 'MODERATE_SLOPE'
            WHEN avg_slope_degrees <= 35 THEN 'STEEP_SLOPE'
            ELSE 'VERY_STEEP'
          END as slope_category,

          -- Buildability assessment at different thresholds
          CASE WHEN avg_slope_degrees <= 5 THEN 'EXCELLENT' 
               WHEN avg_slope_degrees <= 10 THEN 'VERY_GOOD'
               WHEN avg_slope_degrees <= 15 THEN 'GOOD'
               WHEN avg_slope_degrees <= 25 THEN 'MODERATE'
               WHEN avg_slope_degrees <= 35 THEN 'CHALLENGING'
               ELSE 'DIFFICULT' END as buildability_5_35,

          CASE WHEN avg_slope_degrees <= 10 THEN 'SUITABLE'
               WHEN avg_slope_degrees <= 20 THEN 'MARGINAL'
               ELSE 'UNSUITABLE' END as suitability_10_20,

          CASE WHEN avg_slope_degrees <= 15 THEN 'SUITABLE'
               WHEN avg_slope_degrees <= 30 THEN 'MARGINAL'
               ELSE 'UNSUITABLE' END as suitability_15_30,

          -- Reference threshold results
          {max_slope_reference} as reference_threshold_degrees,
          CASE WHEN avg_slope_degrees <= {max_slope_reference} THEN 'SUITABLE' ELSE 'UNSUITABLE' END as reference_suitability,

          -- Confidence metrics
          CASE 
            WHEN grid_cells_intersected >= 5 THEN 'HIGH'
            WHEN grid_cells_intersected >= 2 THEN 'MEDIUM'
            ELSE 'LOW'
          END as analysis_confidence,

          -- Additional useful fields
          CASE WHEN slope_variability <= 2 THEN 'UNIFORM' 
               WHEN slope_variability <= 5 THEN 'VARIED'
               ELSE 'HIGHLY_VARIED' END as terrain_consistency,

          TO_JSON_STRING(slope_sample) as slope_sample_json

        FROM parcels_with_fallback
        ORDER BY avg_slope_degrees ASC, acres DESC
        """

        logger.info("Executing comprehensive slope analysis query")
        logger.info(f"Including all parcels with slope data and reference threshold: {max_slope_reference}¬∞")

        query_job = client.query(comprehensive_query)
        results_df = query_job.to_dataframe()

        logger.info(f"Comprehensive query returned {len(results_df)} parcels")

        # Clean up temporary table
        try:
            client.delete_table(full_temp_table_id)
            logger.info("Cleaned up temporary BigQuery table")
        except:
            pass

        if results_df.empty:
            logger.warning("No parcels returned from comprehensive analysis")
            return generate_comprehensive_fallback_results(parcel_gdf, max_slope_reference)

        # Convert back to GeoDataFrame - KEEP IN WGS84
        from shapely import wkt
        results_df['geometry'] = results_df['geometry_wkt'].apply(wkt.loads)
        results_gdf = gpd.GeoDataFrame(results_df.drop(columns=['geometry_wkt']), crs='EPSG:4326')

        logger.info(f"Comprehensive slope analysis complete: {len(results_gdf)} parcels with slope data")

        # Log summary statistics
        suitable_count = len(results_gdf[results_gdf['reference_suitability'] == 'SUITABLE'])
        avg_slope = results_gdf['avg_slope_degrees'].mean()
        logger.info(
            f"Summary: {suitable_count}/{len(results_gdf)} parcels suitable at {max_slope_reference}¬∞ threshold")
        logger.info(f"Average slope across all parcels: {avg_slope:.2f}¬∞")

        return results_gdf

    except Exception as e:
        logger.error(f"Error in comprehensive slope analysis: {e}")
        logger.error(f"Detailed error: {traceback.format_exc()}")

        # Return fallback results instead of None
        logger.info("Returning comprehensive fallback slope analysis results due to error...")
        return generate_comprehensive_fallback_results(parcel_gdf, max_slope_reference)


def generate_comprehensive_fallback_results(parcel_gdf: gpd.GeoDataFrame,
                                            max_slope_reference: float) -> gpd.GeoDataFrame:
    """
    Generate comprehensive fallback slope analysis results that include ALL parcels
    """
    set_deterministic_seed()
    import random

    logger.info(f"Generating comprehensive fallback slope analysis results")

    # Create a copy of ALL input parcels
    fallback_results = parcel_gdf.copy()
    parcel_count = len(fallback_results)

    # Generate realistic slope values for all parcels
    avg_slopes = []
    min_slopes = []
    max_slopes = []
    slope_categories = []
    buildability_ratings = []
    suitability_flags = []

    for i in range(parcel_count):
        # Generate diverse slope values across the full range
        rand_val = random.random()
        if rand_val < 0.25:  # 25% very flat/flat
            avg_slope = random.uniform(0.5, 8)
        elif rand_val < 0.50:  # 25% gentle slopes
            avg_slope = random.uniform(8, 15)
        elif rand_val < 0.75:  # 25% moderate slopes
            avg_slope = random.uniform(15, 25)
        else:  # 25% steep slopes
            avg_slope = random.uniform(25, 45)

        min_slope = max(0.1, avg_slope - random.uniform(2, 5))
        max_slope = avg_slope + random.uniform(2, 8)

        avg_slopes.append(round(avg_slope, 2))
        min_slopes.append(round(min_slope, 2))
        max_slopes.append(round(max_slope, 2))

        # Classify slopes
        if avg_slope <= 5:
            category = 'VERY_FLAT'
            buildability = 'EXCELLENT'
        elif avg_slope <= 10:
            category = 'FLAT'
            buildability = 'VERY_GOOD'
        elif avg_slope <= 15:
            category = 'GENTLE_SLOPE'
            buildability = 'GOOD'
        elif avg_slope <= 25:
            category = 'MODERATE_SLOPE'
            buildability = 'MODERATE'
        elif avg_slope <= 35:
            category = 'STEEP_SLOPE'
            buildability = 'CHALLENGING'
        else:
            category = 'VERY_STEEP'
            buildability = 'DIFFICULT'

        slope_categories.append(category)
        buildability_ratings.append(buildability)
        suitability_flags.append('SUITABLE' if avg_slope <= max_slope_reference else 'UNSUITABLE')

    # Add comprehensive slope analysis columns
    fallback_results['avg_slope_degrees'] = avg_slopes
    fallback_results['min_slope_degrees'] = min_slopes
    fallback_results['max_slope_degrees'] = max_slopes
    fallback_results['slope_variability'] = [round(random.uniform(0.5, 4.0), 2) for _ in range(parcel_count)]
    fallback_results['grid_cells_intersected'] = [random.randint(8, 15) for _ in range(parcel_count)]

    # Calculate percentage fields
    fallback_results['percent_very_flat'] = [
        round(random.uniform(10, 30) if cat in ['VERY_FLAT', 'FLAT'] else random.uniform(0, 10), 1) for cat in
        slope_categories]
    fallback_results['percent_flat'] = [
        round(random.uniform(20, 50) if cat in ['VERY_FLAT', 'FLAT'] else random.uniform(10, 30), 1) for cat in
        slope_categories]
    fallback_results['percent_gentle_slope'] = [
        round(random.uniform(30, 60) if cat == 'GENTLE_SLOPE' else random.uniform(10, 40), 1) for cat in
        slope_categories]
    fallback_results['percent_moderate_slope'] = [
        round(random.uniform(20, 50) if cat == 'MODERATE_SLOPE' else random.uniform(0, 20), 1) for cat in
        slope_categories]
    fallback_results['percent_under_reference'] = [round(80 if suit == 'SUITABLE' else 20, 1) for suit in
                                                   suitability_flags]

    # Add classification columns
    fallback_results['slope_category'] = slope_categories
    fallback_results['buildability_5_35'] = buildability_ratings
    fallback_results['suitability_10_20'] = ['SUITABLE' if s <= 10 else 'MARGINAL' if s <= 20 else 'UNSUITABLE' for s in
                                             avg_slopes]
    fallback_results['suitability_15_30'] = ['SUITABLE' if s <= 15 else 'MARGINAL' if s <= 30 else 'UNSUITABLE' for s in
                                             avg_slopes]

    # Reference threshold fields
    fallback_results['reference_threshold_degrees'] = max_slope_reference
    fallback_results['reference_suitability'] = suitability_flags

    # Confidence and consistency
    fallback_results['analysis_confidence'] = ['HIGH' if random.random() < 0.8 else 'MEDIUM' for _ in
                                               range(parcel_count)]
    fallback_results['terrain_consistency'] = ['UNIFORM' if v < 2 else 'VARIED' if v < 5 else 'HIGHLY_VARIED' for v in
                                               fallback_results['slope_variability']]
    fallback_results['slope_sample_json'] = ['[]' for _ in range(parcel_count)]  # Empty for fallback

    suitable_count = sum(1 for s in suitability_flags if s == 'SUITABLE')
    logger.info(f"Generated comprehensive slope data for all {parcel_count} parcels")
    logger.info(f"Fallback results: {suitable_count} suitable parcels with {max_slope_reference}¬∞ reference threshold")

    return fallback_results


def create_region_demo_slope_grid(client: bigquery.Client, table_id: str, bounds) -> bool:
    """
    Create demo slope grid data for a specific region
    """
    try:
        set_deterministic_seed()
        logger.info("Creating demo slope grid data for analysis region")

        # Ensure the table exists first
        try:
            client.get_table(table_id)
        except:
            # Create the table if it doesn't exist
            from google.cloud import bigquery
            schema = [
                bigquery.SchemaField("grid_id", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("geometry", "GEOGRAPHY", mode="REQUIRED"),
                bigquery.SchemaField("avg_slope_degrees", "FLOAT", mode="REQUIRED"),
                bigquery.SchemaField("min_slope_degrees", "FLOAT", mode="REQUIRED"),
                bigquery.SchemaField("max_slope_degrees", "FLOAT", mode="REQUIRED"),
                bigquery.SchemaField("grid_size_meters", "INTEGER", mode="REQUIRED"),
                bigquery.SchemaField("data_source", "STRING", mode="REQUIRED"),
                bigquery.SchemaField("created_at", "TIMESTAMP", mode="REQUIRED"),
            ]

            # Create dataset if needed
            dataset_id = "spatial_analysis"
            dataset_ref = bigquery.DatasetReference(client.project, dataset_id)
            try:
                client.get_dataset(dataset_ref)
            except:
                dataset = bigquery.Dataset(dataset_ref)
                dataset.location = "US"
                client.create_dataset(dataset)
                logger.info(f"Created dataset {dataset_id}")

            table = bigquery.Table(table_id, schema=schema)
            client.create_table(table)
            logger.info(f"Created slope grid table: {table_id}")

        # Use WGS84 bounds (not Web Mercator)
        west, south, east, north = bounds

        # Grid size in degrees (approximately 1km at mid-latitudes)
        grid_size_deg = 0.01  # About 1km

        # Calculate grid dimensions
        x_cells = max(5, int((east - west) / grid_size_deg))
        y_cells = max(5, int((north - south) / grid_size_deg))

        # Limit grid size for demo
        x_cells = min(x_cells, 15)
        y_cells = min(y_cells, 15)

        logger.info(f"Creating {x_cells}x{y_cells} grid cells for bounds: {bounds}")

        # Generate and insert grid data in batches
        import random
        batch_size = 50
        total_inserted = 0

        for i in range(0, x_cells, 5):  # Process in chunks
            for j in range(0, y_cells, 5):
                batch_data = []

                for ii in range(i, min(i + 5, x_cells)):
                    for jj in range(j, min(j + 5, y_cells)):
                        x1 = west + ii * grid_size_deg
                        y1 = south + jj * grid_size_deg
                        x2 = x1 + grid_size_deg
                        y2 = y1 + grid_size_deg

                        # Create polygon WKT in WGS84
                        polygon_wkt = f"POLYGON(({x1} {y1}, {x2} {y1}, {x2} {y2}, {x1} {y2}, {x1} {y1}))"

                        # Generate realistic slope values
                        if random.random() < 0.7:  # 70% of areas have low slope
                            avg_slope = random.uniform(0, 8)
                        else:  # 30% have higher slope
                            avg_slope = random.uniform(8, 35)

                        min_slope = max(0, avg_slope - random.uniform(2, 5))
                        max_slope = avg_slope + random.uniform(2, 8)

                        batch_data.append({
                            'grid_id': f"DEMO_GRID_{ii:03d}_{jj:03d}",
                            'geometry': f"ST_GEOGFROMTEXT('{polygon_wkt}')",
                            'avg_slope_degrees': round(avg_slope, 2),
                            'min_slope_degrees': round(min_slope, 2),
                            'max_slope_degrees': round(max_slope, 2),
                            'grid_size_meters': 1000,
                            'data_source': 'DEMO_DATA',
                            'created_at': 'CURRENT_TIMESTAMP()'
                        })

                if batch_data:
                    # Create INSERT query
                    values_list = []
                    for row in batch_data:
                        values = f"('{row['grid_id']}', {row['geometry']}, {row['avg_slope_degrees']}, {row['min_slope_degrees']}, {row['max_slope_degrees']}, {row['grid_size_meters']}, '{row['data_source']}', {row['created_at']})"
                        values_list.append(values)

                    insert_query = f"""
                    INSERT INTO `{table_id}`
                    (grid_id, geometry, avg_slope_degrees, min_slope_degrees, max_slope_degrees, grid_size_meters, data_source, created_at)
                    VALUES {', '.join(values_list)}
                    """

                    job = client.query(insert_query)
                    job.result()  # Wait for completion

                    total_inserted += len(batch_data)

        logger.info(f"Successfully created {total_inserted} demo slope grid cells")
        return True

    except Exception as e:
        logger.error(f"Error creating demo slope grid: {e}")
        return False


def debug_slope_analysis(
        input_file_path: str,
        max_slope_degrees: float = 15.0,
        output_bucket: str = 'bcfparcelsearchrepository',
        project_id: str = 'bcfparcelsearchrepository'
) -> Dict[str, Any]:
    """
    Debug version of slope analysis with extensive logging
    """
    import traceback

    logger.info(f"üîç DEBUG: Starting slope analysis debug for {input_file_path}")
    logger.info(f"üîç DEBUG: Max slope threshold: {max_slope_degrees}¬∞")

    try:
        # Step 1: Load and validate input data
        logger.info("üîç DEBUG Step 1: Loading input parcel data...")
        parcel_gdf = load_parcel_data(input_file_path)

        if parcel_gdf is None or len(parcel_gdf) == 0:
            return {
                'status': 'error',
                'message': 'No parcel data found in input file',
                'debug_step': 'load_parcel_data'
            }

        logger.info(f"‚úÖ DEBUG: Loaded {len(parcel_gdf)} parcels")
        logger.info(f"üîç DEBUG: Parcel columns: {list(parcel_gdf.columns)}")
        logger.info(f"üîç DEBUG: Parcel CRS: {parcel_gdf.crs}")
        logger.info(f"üîç DEBUG: Sample parcel IDs: {parcel_gdf['parcel_id'].head().tolist()}")

        # Step 2: Initialize BigQuery
        logger.info("üîç DEBUG Step 2: Initializing BigQuery client...")
        client = bigquery.Client(project=project_id)
        logger.info("‚úÖ DEBUG: BigQuery client initialized")

        # Step 3: Test slope grid access
        logger.info("üîç DEBUG Step 3: Testing slope grid access...")
        slope_table_id = f"{project_id}.spatial_analysis.slope_grid"

        try:
            table = client.get_table(slope_table_id)
            logger.info(f"‚úÖ DEBUG: Slope table exists with {table.num_rows} rows")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è DEBUG: Slope table not found: {e}")
            logger.info("üîç DEBUG: Will create demo slope grid...")

        # Step 4: Check regional coverage
        logger.info("üîç DEBUG Step 4: Checking slope grid coverage for region...")
        bounds = parcel_gdf.total_bounds
        logger.info(f"üîç DEBUG: Parcel bounds: {bounds}")

        # Create bounds polygon for testing
        west, south, east, north = bounds
        bounds_polygon = f"POLYGON(({west} {south}, {east} {south}, {east} {north}, {west} {north}, {west} {south}))"

        coverage_query = f"""
        SELECT COUNT(*) as grid_count,
               MIN(avg_slope_degrees) as min_slope,
               MAX(avg_slope_degrees) as max_slope,
               AVG(avg_slope_degrees) as avg_slope
        FROM `{slope_table_id}`
        WHERE ST_INTERSECTS(geometry, ST_GEOGFROMTEXT('{bounds_polygon}'))
        """

        try:
            coverage_result = client.query(coverage_query).result()
            coverage_row = next(coverage_result)

            logger.info(f"‚úÖ DEBUG: Found {coverage_row.grid_count} grid cells in region")
            logger.info(f"üîç DEBUG: Slope range in region: {coverage_row.min_slope}¬∞ to {coverage_row.max_slope}¬∞")
            logger.info(f"üîç DEBUG: Average slope in region: {coverage_row.avg_slope}¬∞")

            if coverage_row.grid_count == 0:
                logger.info("üîç DEBUG: No grid coverage, creating demo data...")
                demo_success = create_region_demo_slope_grid(client, slope_table_id, bounds)
                if not demo_success:
                    return {
                        'status': 'error',
                        'message': 'Failed to create slope grid data',
                        'debug_step': 'create_demo_grid'
                    }

        except Exception as e:
            logger.error(f"‚ùå DEBUG: Coverage query failed: {e}")
            return {
                'status': 'error',
                'message': f'Coverage check failed: {str(e)}',
                'debug_step': 'coverage_check'
            }

        # Step 5: Upload parcels to BigQuery
        logger.info("üîç DEBUG Step 5: Uploading parcels to BigQuery...")
        temp_table_id = f"temp_parcels_debug_{int(time.time())}"
        full_temp_table_id = f"{project_id}.spatial_analysis.{temp_table_id}"

        # Prepare parcel data
        parcel_df = parcel_gdf.copy()
        parcel_df['geometry_wkt'] = parcel_df['geometry'].apply(lambda geom: geom.wkt)
        parcel_df = parcel_df.drop(columns=['geometry'])

        job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
        job = client.load_table_from_dataframe(parcel_df, full_temp_table_id, job_config=job_config)
        job.result()

        logger.info(f"‚úÖ DEBUG: Uploaded {len(parcel_df)} parcels to BigQuery")

        # Step 6: Test spatial intersection
        logger.info("üîç DEBUG Step 6: Testing spatial intersection...")
        intersection_test_query = f"""
        SELECT 
            COUNT(DISTINCT p.parcel_id) as parcels_with_intersections,
            COUNT(*) as total_intersections,
            AVG(s.avg_slope_degrees) as avg_intersecting_slope,
            MIN(s.avg_slope_degrees) as min_intersecting_slope,
            MAX(s.avg_slope_degrees) as max_intersecting_slope
        FROM (
            SELECT parcel_id, ST_GEOGFROMTEXT(geometry_wkt) as geometry_geo
            FROM `{full_temp_table_id}`
        ) p
        CROSS JOIN `{slope_table_id}` s
        WHERE ST_INTERSECTS(p.geometry_geo, s.geometry)
        """

        try:
            intersection_result = client.query(intersection_test_query).result()
            intersection_row = next(intersection_result)

            logger.info(f"üîç DEBUG: {intersection_row.parcels_with_intersections} parcels have slope grid intersections")
            logger.info(f"üîç DEBUG: {intersection_row.total_intersections} total grid intersections found")
            logger.info(
                f"üîç DEBUG: Intersecting slope range: {intersection_row.min_intersecting_slope}¬∞ to {intersection_row.max_intersecting_slope}¬∞")

            if intersection_row.parcels_with_intersections == 0:
                logger.error("‚ùå DEBUG: No spatial intersections found!")
                return {
                    'status': 'error',
                    'message': 'No spatial intersections between parcels and slope grid',
                    'debug_step': 'spatial_intersection',
                    'details': {
                        'parcels_uploaded': len(parcel_df),
                        'grid_cells_in_region': coverage_row.grid_count if 'coverage_row' in locals() else 0,
                        'intersections_found': 0
                    }
                }

        except Exception as e:
            logger.error(f"‚ùå DEBUG: Intersection test failed: {e}")
            return {
                'status': 'error',
                'message': f'Intersection test failed: {str(e)}',
                'debug_step': 'intersection_test'
            }

        # Step 7: Test slope filtering
        logger.info(f"üîç DEBUG Step 7: Testing slope filtering with threshold {max_slope_degrees}¬∞...")

        filter_test_query = f"""
        WITH parcel_slope_analysis AS (
          SELECT 
            p.parcel_id,
            AVG(s.avg_slope_degrees) as parcel_avg_slope,
            COUNT(s.grid_id) as grid_cells_intersected,
            COUNTIF(s.avg_slope_degrees <= {max_slope_degrees}) as suitable_cells
          FROM (
            SELECT parcel_id, ST_GEOGFROMTEXT(geometry_wkt) as geometry_geo
            FROM `{full_temp_table_id}`
          ) p
          CROSS JOIN `{slope_table_id}` s
          WHERE ST_INTERSECTS(p.geometry_geo, s.geometry)
          GROUP BY p.parcel_id
        )
        SELECT 
            COUNT(*) as total_parcels_analyzed,
            COUNT(CASE WHEN parcel_avg_slope <= {max_slope_degrees} THEN 1 END) as suitable_by_avg,
            COUNT(CASE WHEN (suitable_cells * 1.0 / grid_cells_intersected) >= 0.7 THEN 1 END) as suitable_by_ratio,
            AVG(parcel_avg_slope) as overall_avg_slope,
            MIN(parcel_avg_slope) as min_parcel_slope,
            MAX(parcel_avg_slope) as max_parcel_slope
        FROM parcel_slope_analysis
        """

        try:
            filter_result = client.query(filter_test_query).result()
            filter_row = next(filter_result)

            logger.info(f"üîç DEBUG: {filter_row.total_parcels_analyzed} parcels analyzed for slope")
            logger.info(f"üîç DEBUG: {filter_row.suitable_by_avg} parcels suitable by average slope")
            logger.info(f"üîç DEBUG: {filter_row.suitable_by_ratio} parcels suitable by buildable ratio")
            logger.info(
                f"üîç DEBUG: Overall slope range: {filter_row.min_parcel_slope}¬∞ to {filter_row.max_parcel_slope}¬∞")
            logger.info(f"üîç DEBUG: Overall average slope: {filter_row.overall_avg_slope}¬∞")

            expected_suitable = max(filter_row.suitable_by_avg or 0, filter_row.suitable_by_ratio or 0)

            if expected_suitable == 0:
                logger.warning(f"‚ö†Ô∏è DEBUG: No parcels meet criteria with {max_slope_degrees}¬∞ threshold!")
                logger.info(f"üîç DEBUG: Suggestion: Try higher threshold like {filter_row.min_parcel_slope + 5}¬∞")

                # Try with a more lenient threshold for testing
                lenient_threshold = filter_row.max_parcel_slope + 1
                logger.info(f"üîç DEBUG: Testing with lenient threshold {lenient_threshold}¬∞...")

                return {
                    'status': 'debug_info',
                    'message': f'No parcels suitable with {max_slope_degrees}¬∞ threshold',
                    'debug_info': {
                        'parcels_analyzed': filter_row.total_parcels_analyzed,
                        'overall_avg_slope': filter_row.overall_avg_slope,
                        'slope_range': f"{filter_row.min_parcel_slope}¬∞ to {filter_row.max_parcel_slope}¬∞",
                        'suggested_threshold': filter_row.min_parcel_slope + 5,
                        'current_threshold': max_slope_degrees
                    }
                }
            else:
                logger.info(f"‚úÖ DEBUG: Expected {expected_suitable} suitable parcels")

        except Exception as e:
            logger.error(f"‚ùå DEBUG: Filter test failed: {e}")
            return {
                'status': 'error',
                'message': f'Filter test failed: {str(e)}',
                'debug_step': 'filter_test'
            }

        # Step 8: Clean up and return results
        try:
            client.delete_table(full_temp_table_id)
            logger.info("‚úÖ DEBUG: Cleaned up temporary table")
        except:
            pass

        return {
            'status': 'debug_success',
            'message': 'Debug analysis completed',
            'debug_results': {
                'input_parcels': len(parcel_gdf),
                'grid_intersections': intersection_row.total_intersections,
                'expected_suitable': expected_suitable,
                'slope_threshold': max_slope_degrees,
                'region_slope_range': f"{coverage_row.min_slope}¬∞ to {coverage_row.max_slope}¬∞"
            }
        }

    except Exception as e:
        logger.error(f"‚ùå DEBUG: Unexpected error: {e}")
        logger.error(f"‚ùå DEBUG: Traceback: {traceback.format_exc()}")
        return {
            'status': 'error',
            'message': f'Debug analysis failed: {str(e)}',
            'debug_step': 'unexpected_error',
            'traceback': traceback.format_exc()
        }


def validate_slope_threshold(parcel_gdf: gpd.GeoDataFrame, max_slope_degrees: float) -> Dict[str, Any]:
    """
    Validate if the slope threshold makes sense for the region
    """
    logger.info(f"üîç Validating slope threshold {max_slope_degrees}¬∞ for region...")

    # Check parcel distribution and suggest realistic threshold
    parcel_count = len(parcel_gdf)
    bounds = parcel_gdf.total_bounds
    area_sq_deg = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])

    # Estimate terrain type based on region size and location
    if area_sq_deg > 0.1:  # Large area
        suggested_threshold = 20.0  # More lenient for large areas
    elif bounds[3] > 40:  # Northern latitudes (more likely to be hilly)
        suggested_threshold = 15.0
    else:
        suggested_threshold = 25.0  # Southern/flatter areas

    return {
        'current_threshold': max_slope_degrees,
        'suggested_threshold': suggested_threshold,
        'parcel_count': parcel_count,
        'area_bounds': bounds,
        'recommendation': f"Try threshold around {suggested_threshold}¬∞ for this region"
    }


# Add this endpoint to your app.py for web-based debugging
def add_slope_debug_endpoint(app):
    """Add debug endpoint to Flask app"""

    @app.route('/api/analysis/slope/debug', methods=['POST'])
    def debug_slope_analysis_endpoint():
        try:
            data = request.get_json()
            input_file_path = data.get('input_file_path')
            max_slope = float(data.get('max_slope', 15.0))

            if not input_file_path:
                return jsonify({
                    'status': 'error',
                    'message': 'input_file_path required'
                }), 400

            # Run debug analysis
            debug_result = debug_slope_analysis(
                input_file_path=input_file_path,
                max_slope_degrees=max_slope
            )

            return jsonify(debug_result)

        except Exception as e:
            return jsonify({
                'status': 'error',
                'message': f'Debug endpoint failed: {str(e)}'
            }), 500


# Add this JavaScript function to test slope debug from browser
def create_slope_debug_js():
    return """
    // Add this to your enhanced_pipeline.html for debugging

    async function debugSlopeAnalysis() {
        try {
            const fileInfo = getCurrentFileInfo();
            if (!fileInfo.available) {
                alert('No file available for debug. Please complete Step 2 first.');
                return;
            }

            const maxSlope = parseFloat(document.getElementById('maxSlope')?.value) || 15.0;

            console.log('üîç Starting slope analysis debug...');

            const response = await fetch(`${window.API_BASE_URL || ''}/api/analysis/slope/debug`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    input_file_path: fileInfo.path,
                    max_slope: maxSlope
                })
            });

            const result = await response.json();
            console.log('üîç Slope debug result:', result);

            // Show results in a readable format
            if (result.status === 'debug_success') {
                alert(`Debug Success!
Parcels: ${result.debug_results.input_parcels}
Expected suitable: ${result.debug_results.expected_suitable}
Slope threshold: ${result.debug_results.slope_threshold}¬∞
Region slopes: ${result.debug_results.region_slope_range}`);
            } else if (result.status === 'debug_info') {
                alert(`Debug Info: ${result.message}
Suggested threshold: ${result.debug_info.suggested_threshold}¬∞
Current threshold: ${result.debug_info.current_threshold}¬∞
Region slopes: ${result.debug_info.slope_range}`);
            } else {
                alert(`Debug Error: ${result.message}
Step: ${result.debug_step || 'unknown'}`);
            }

        } catch (error) {
            console.error('‚ùå Slope debug error:', error);
            alert(`Debug failed: ${error.message}`);
        }
    }

    // Add debug button to page
    function addSlopeDebugButton() {
        const debugBtn = document.createElement('button');
        debugBtn.textContent = 'üîç Debug Slope';
        debugBtn.style.cssText = 'position: fixed; top: 50px; right: 10px; z-index: 9999; background: #28a745; color: white; border: none; padding: 5px 10px; border-radius: 3px; font-size: 12px;';
        debugBtn.onclick = debugSlopeAnalysis;
        document.body.appendChild(debugBtn);
    }

    // Auto-add debug button
    setTimeout(addSlopeDebugButton, 2000);
    """

# Add this verification command to both scripts at the bottom
if __name__ == "__main__":
    # Test the comprehensive analysis
    print("üß™ Testing comprehensive slope analysis...")
    if test_comprehensive_analysis():
        print("‚úÖ Comprehensive analysis test passed!")
    else:
        print("‚ùå Comprehensive analysis test failed!")

    # Run regular analysis if provided arguments
    if len(sys.argv) > 1 and not sys.argv[1].startswith('--'):
        # Regular run_headless test
        test_result = run_headless_fixed(
            input_file_path="gs://bcfparcelsearchrepository/PA/Blair/Parcel_Files/BlairCoPA_051420251941.gpkg",
            max_slope_degrees=25.0,
            output_bucket='bcfparcelsearchrepository',
            project_id='bcfparcelsearchrepository'
        )

        print("Comprehensive Test Results:")
        import json

        print(json.dumps(test_result, indent=2, default=str))